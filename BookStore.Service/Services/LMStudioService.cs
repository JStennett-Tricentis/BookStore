using System.Diagnostics;
using System.Diagnostics.Metrics;
using BookStore.Common.Instrumentation;
using OpenAI.Chat;

namespace BookStore.Service.Services;

/// <summary>
/// Service for generating book summaries using LM Studio (local LLM server with OpenAI-compatible API).
/// LM Studio provides access to hundreds of models with a simple OpenAI-compatible interface.
/// Provides free, unlimited API calls for performance testing without cost concerns.
/// </summary>
public class LMStudioService : ILLMService
{
    private readonly HttpClient _httpClient;
    private readonly string _baseUrl;
    private readonly ILogger<LMStudioService> _logger;
    private readonly ActivitySource _activitySource;
    private readonly Counter<long> _inputTokensCounter;
    private readonly Counter<long> _outputTokensCounter;
    private readonly Counter<long> _totalTokensCounter;
    private readonly Histogram<double> _costHistogram;
    private readonly string _model;

    public string ProviderName => "lmstudio";

    public LMStudioService(
        IConfiguration configuration,
        ILogger<LMStudioService> logger,
        ActivitySource activitySource,
        IMeterFactory meterFactory,
        IHttpClientFactory httpClientFactory)
    {
        _baseUrl = configuration["LLM:Providers:LMStudio:Url"] ?? "http://localhost:1234";
        _model = configuration["LLM:Providers:LMStudio:Model"] ?? "local-model";

        // Use HttpClient directly for better compatibility with LM Studio
        _httpClient = httpClientFactory.CreateClient();
        _httpClient.BaseAddress = new Uri(_baseUrl);
        _httpClient.Timeout = TimeSpan.FromMinutes(2);

        _logger = logger;
        _activitySource = activitySource;

        // Create meter for LM Studio metrics
        var meter = meterFactory.Create("BookStore.Service.LMStudio");

        _inputTokensCounter = meter.CreateCounter<long>(
            "lmstudio.tokens.input",
            unit: "tokens",
            description: "Number of input tokens consumed by LM Studio");

        _outputTokensCounter = meter.CreateCounter<long>(
            "lmstudio.tokens.output",
            unit: "tokens",
            description: "Number of output tokens generated by LM Studio");

        _totalTokensCounter = meter.CreateCounter<long>(
            "lmstudio.tokens.total",
            unit: "tokens",
            description: "Total number of tokens (input + output) used by LM Studio");

        _costHistogram = meter.CreateHistogram<double>(
            "lmstudio.cost.usd",
            unit: "USD",
            description: "Estimated cost per request in USD (always $0 for LM Studio)");
    }

    public async Task<string> GenerateBookSummaryAsync(
        string title,
        string author,
        string? description,
        CancellationToken cancellationToken = default)
    {
        using var activity = _activitySource.StartActivity("lmstudio.generate_summary", ActivityKind.Client);

        var prompt = BuildPrompt(title, author, description);

        // Add LLM-specific trace tags using OpenTelemetry semantic conventions
        activity?.SetTag(TraceTags.LlmRequestTypeKey, TraceTags.CompletionRequestType);
        activity?.SetTag(TraceTags.LlmOperationNameKey, TraceTags.CompletionOperation);
        activity?.SetTag(TraceTags.GenAiOperationNameKey, TraceTags.CompletionOperation);
        activity?.SetTag(TraceTags.LLMSystem, "lmstudio");
        activity?.SetTag(TraceTags.LlmModelNameKey, _model);
        activity?.SetTag(TraceTags.LlmPrompt0ContentKey, prompt);
        activity?.SetTag(TraceTags.LlmPrompt0RoleKey, "user");

        var startTime = DateTimeOffset.UtcNow;

        try
        {
            // Build OpenAI-compatible request
            var requestBody = new
            {
                model = _model,
                messages = new[]
                {
                    new { role = "system", content = "You are a helpful assistant that generates concise book summaries." },
                    new { role = "user", content = prompt }
                },
                max_tokens = 500,
                temperature = 0.7,
                stream = false
            };

            var json = System.Text.Json.JsonSerializer.Serialize(requestBody);
            var content = new StringContent(json, System.Text.Encoding.UTF8, "application/json");

            // Call LM Studio API
            var response = await _httpClient.PostAsync("/v1/chat/completions", content, cancellationToken);
            response.EnsureSuccessStatusCode();

            var responseJson = await response.Content.ReadAsStringAsync(cancellationToken);
            var latency = (DateTimeOffset.UtcNow - startTime).TotalMilliseconds;

            // Parse response
            using var jsonDoc = System.Text.Json.JsonDocument.Parse(responseJson);
            var root = jsonDoc.RootElement;

            // Extract summary from choices[0].message.content
            var summary = "Unable to generate summary";
            if (root.TryGetProperty("choices", out var choices) && choices.GetArrayLength() > 0)
            {
                var firstChoice = choices[0];
                if (firstChoice.TryGetProperty("message", out var message) &&
                    message.TryGetProperty("content", out var contentElement))
                {
                    summary = contentElement.GetString()?.Trim() ?? "Unable to generate summary";
                }
            }

            // Extract token usage
            var inputTokens = 0;
            var outputTokens = 0;
            var totalTokens = 0;

            if (root.TryGetProperty("usage", out var usage))
            {
                if (usage.TryGetProperty("prompt_tokens", out var promptTokens))
                    inputTokens = promptTokens.GetInt32();
                if (usage.TryGetProperty("completion_tokens", out var completionTokens))
                    outputTokens = completionTokens.GetInt32();
                if (usage.TryGetProperty("total_tokens", out var total))
                    totalTokens = total.GetInt32();
            }

            // LM Studio is free - cost is always $0
            const double totalCost = 0.0;

            // Record metrics
            _inputTokensCounter.Add(inputTokens, new KeyValuePair<string, object?>("model", _model));
            _outputTokensCounter.Add(outputTokens, new KeyValuePair<string, object?>("model", _model));
            _totalTokensCounter.Add(totalTokens, new KeyValuePair<string, object?>("model", _model));
            _costHistogram.Record(totalCost, new KeyValuePair<string, object?>("model", _model));

            // Add response trace tags
            activity?.SetTag(TraceTags.GenAiResponseModelKey, _model);
            activity?.SetTag(TraceTags.GenAiUsageInputTokensKey, inputTokens);
            activity?.SetTag(TraceTags.GenAiUsageOutputTokensKey, outputTokens);
            activity?.SetTag(TraceTags.GenAiUsageTotalTokensKey, totalTokens);
            activity?.SetTag(TraceTags.LlmLatencyKey, latency);
            activity?.SetTag(TraceTags.LlmCompletion0ContentKey, summary);
            activity?.SetTag(TraceTags.LlmCompletion0RoleKey, "assistant");
            activity?.SetTag(TraceTags.LlmCompletion0FinishReasonKey, "stop");  // LM Studio returns "stop" for normal completion
            activity?.SetTag(TraceTags.TraceLoopOutputKey, summary);
            activity?.SetTag("gen_ai.cost.usd", totalCost);
            activity?.SetStatus(ActivityStatusCode.Ok);

            _logger.LogInformation(
                "Generated book summary using LM Studio. Model: {Model}, Input tokens: {InputTokens}, Output tokens: {OutputTokens}, Cost: $0 (FREE), Latency: {Latency}ms",
                _model, inputTokens, outputTokens, latency);

            return summary;
        }
        catch (Exception ex)
        {
            activity?.SetStatus(ActivityStatusCode.Error, ex.Message);
            activity?.AddTag("exception.type", ex.GetType().FullName);
            activity?.AddTag("exception.message", ex.Message);
            _logger.LogError(ex, "Failed to generate book summary using LM Studio");
            throw;
        }
    }

    private static string BuildPrompt(string title, string author, string? description)
    {
        var prompt = $"Generate a concise, engaging 2-3 sentence summary for a book titled \"{title}\" by {author}.";

        if (!string.IsNullOrEmpty(description))
        {
            prompt += $" Here's some context about the book: {description}";
        }

        prompt += " Focus on what makes this book interesting and worth reading. Provide ONLY the summary, no additional commentary.";

        return prompt;
    }
}
