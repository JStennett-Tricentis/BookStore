using System.Diagnostics;
using System.Diagnostics.Metrics;
using BookStore.Common.Instrumentation;
using OllamaSharp;
using OllamaSharp.Models;

namespace BookStore.Service.Services;

/// <summary>
/// Service for generating book summaries using Ollama (local LLM models).
/// Provides free, unlimited API calls for performance testing without cost concerns.
/// </summary>
public class OllamaService : ILLMService
{
    private readonly OllamaApiClient _client;
    private readonly ILogger<OllamaService> _logger;
    private readonly ActivitySource _activitySource;
    private readonly Counter<long> _inputTokensCounter;
    private readonly Counter<long> _outputTokensCounter;
    private readonly Counter<long> _totalTokensCounter;
    private readonly Histogram<double> _costHistogram;
    private readonly string _model;

    public string ProviderName => "ollama";

    public OllamaService(
        IConfiguration configuration,
        ILogger<OllamaService> logger,
        ActivitySource activitySource,
        IMeterFactory meterFactory)
    {
        var ollamaUrl = configuration["LLM:Providers:Ollama:Url"] ?? configuration["Ollama:Url"] ?? configuration["Ollama:BaseUrl"] ?? "http://localhost:11434";
        _model = configuration["LLM:Providers:Ollama:Model"] ?? configuration["Ollama:Model"] ?? "llama3.2";  // Default model
        _client = new OllamaApiClient(new Uri(ollamaUrl), _model);
        _logger = logger;
        _activitySource = activitySource;

        // Create meter for Ollama metrics
        var meter = meterFactory.Create("BookStore.Service.Ollama");

        _inputTokensCounter = meter.CreateCounter<long>(
            "ollama.tokens.input",
            unit: "tokens",
            description: "Number of input tokens consumed by Ollama");

        _outputTokensCounter = meter.CreateCounter<long>(
            "ollama.tokens.output",
            unit: "tokens",
            description: "Number of output tokens generated by Ollama");

        _totalTokensCounter = meter.CreateCounter<long>(
            "ollama.tokens.total",
            unit: "tokens",
            description: "Total number of tokens (input + output) used by Ollama");

        _costHistogram = meter.CreateHistogram<double>(
            "ollama.cost.usd",
            unit: "USD",
            description: "Estimated cost per request in USD (always $0 for Ollama)");
    }

    public async Task<string> GenerateBookSummaryAsync(
        string title,
        string author,
        string? description,
        CancellationToken cancellationToken = default)
    {
        using var activity = _activitySource.StartActivity("ollama.generate_summary", ActivityKind.Client);

        var prompt = BuildPrompt(title, author, description);

        // Add LLM-specific trace tags using OpenTelemetry semantic conventions
        activity?.SetTag(TraceTags.LlmRequestTypeKey, TraceTags.CompletionRequestType);
        activity?.SetTag(TraceTags.LlmOperationNameKey, TraceTags.CompletionOperation);
        activity?.SetTag(TraceTags.GenAiOperationNameKey, TraceTags.CompletionOperation);
        activity?.SetTag(TraceTags.LLMSystem, "ollama");
        activity?.SetTag(TraceTags.LlmModelNameKey, _model);
        activity?.SetTag(TraceTags.LlmPrompt0ContentKey, prompt);
        activity?.SetTag(TraceTags.LlmPrompt0RoleKey, "user");

        var startTime = DateTimeOffset.UtcNow;

        try
        {
            var request = new GenerateRequest
            {
                Prompt = prompt,
                Model = _model,
                Stream = false
            };

            var responseStream = _client.Generate(request, cancellationToken);
            var summaryBuilder = new System.Text.StringBuilder();
            GenerateResponseStream? lastChunk = null;

            await foreach (var chunk in responseStream)
            {
                if (chunk?.Response != null)
                {
                    summaryBuilder.Append(chunk.Response);
                }
                lastChunk = chunk;
            }

            var latency = (DateTimeOffset.UtcNow - startTime).TotalMilliseconds;
            var summary = summaryBuilder.ToString().Trim();
            if (string.IsNullOrEmpty(summary))
                summary = "Unable to generate summary";

            // Estimate token counts (Ollama streaming doesn't provide count details)
            var inputTokens = EstimateTokenCount(prompt);
            var outputTokens = EstimateTokenCount(summary);
            var totalTokens = inputTokens + outputTokens;

            // Ollama is free - cost is always $0
            const double totalCost = 0.0;

            // Record metrics
            _inputTokensCounter.Add(inputTokens, new KeyValuePair<string, object?>("model", _model));
            _outputTokensCounter.Add(outputTokens, new KeyValuePair<string, object?>("model", _model));
            _totalTokensCounter.Add(totalTokens, new KeyValuePair<string, object?>("model", _model));
            _costHistogram.Record(totalCost, new KeyValuePair<string, object?>("model", _model));

            // Add response trace tags
            activity?.SetTag(TraceTags.GenAiResponseModelKey, _model);
            activity?.SetTag(TraceTags.GenAiUsageInputTokensKey, inputTokens);
            activity?.SetTag(TraceTags.GenAiUsageOutputTokensKey, outputTokens);
            activity?.SetTag(TraceTags.GenAiUsageTotalTokensKey, totalTokens);
            activity?.SetTag(TraceTags.LlmLatencyKey, latency);
            activity?.SetTag(TraceTags.LlmCompletion0ContentKey, summary);
            activity?.SetTag(TraceTags.LlmCompletion0RoleKey, "assistant");
            activity?.SetTag(TraceTags.LlmCompletion0FinishReasonKey, lastChunk?.Done == true ? "stop" : "length");
            activity?.SetTag(TraceTags.TraceLoopOutputKey, summary);
            activity?.SetTag("gen_ai.cost.usd", totalCost);
            activity?.SetStatus(ActivityStatusCode.Ok);

            _logger.LogInformation(
                "Generated book summary using Ollama. Model: {Model}, Input tokens: {InputTokens}, Output tokens: {OutputTokens}, Cost: $0 (FREE), Latency: {Latency}ms",
                _model, inputTokens, outputTokens, latency);

            return summary;
        }
        catch (Exception ex)
        {
            activity?.SetStatus(ActivityStatusCode.Error, ex.Message);
            activity?.AddTag("exception.type", ex.GetType().FullName);
            activity?.AddTag("exception.message", ex.Message);
            _logger.LogError(ex, "Failed to generate book summary using Ollama");
            throw;
        }
    }

    private static string BuildPrompt(string title, string author, string? description)
    {
        var prompt = $"Generate a concise, engaging 2-3 sentence summary for a book titled \"{title}\" by {author}.";

        if (!string.IsNullOrEmpty(description))
        {
            prompt += $" Here's some context about the book: {description}";
        }

        prompt += " Focus on what makes this book interesting and worth reading. Provide ONLY the summary, no additional commentary.";

        return prompt;
    }

    /// <summary>
    /// Estimates token count when Ollama doesn't provide it.
    /// Rough approximation: ~4 characters per token for English text.
    /// </summary>
    private static int EstimateTokenCount(string text)
    {
        if (string.IsNullOrEmpty(text))
            return 0;

        // Approximate: 1 token â‰ˆ 4 characters or 0.75 words
        return (int)Math.Ceiling(text.Length / 4.0);
    }
}
